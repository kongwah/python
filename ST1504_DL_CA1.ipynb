{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ST1504_DL_CA1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhGt0mZ8z2SZ"
      },
      "source": [
        "# import the libraries\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "import numpy as np\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import np_utils\n",
        "from keras.layers import ZeroPadding2D, GlobalAveragePooling2D\n",
        "# from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn9y8C9VMGam"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "# load (downloaded if needed) the MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XVGuhRlMSHm"
      },
      "source": [
        "# Display the 1st 20 digits of the train set\n",
        "# what is the format or representation - which is width or height?\n",
        "print(X_train.shape)\n",
        "X_train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6bEZsX7MX8d"
      },
      "source": [
        "for i in range(20):\n",
        "  plt.imshow(X_train[i], cmap=plt.get_cmap('gray'))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjYCk1M3H_9Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LafZ44R-05JK",
        "outputId": "be6dd5af-5242-431a-963a-017e1cd91803"
      },
      "source": [
        "# A simple test\n",
        "# This model has no hidden layer.\n",
        "# It only has an output layer (of 10 neurons) directly connected to the input layer (of 784 neurons)\n",
        "# fix random seed for reproducibility\n",
        "seed = 88\n",
        "numpy.random.seed(seed)\n",
        "# load data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# flatten 28*28 images to a 784 vector for each image\n",
        "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
        "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n",
        "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\n",
        "\n",
        "# normalize inputs from 0-255 to 0-1\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "# one hot encode outputs\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]\n",
        "print(num_classes)\n",
        "print(num_pixels)\n",
        "\n",
        "\n",
        "# build the model\n",
        "# create model with no hidden layer\n",
        "model = Sequential()\n",
        "# this is the output layer (10 neurons, denoting the 10 digit classes)\n",
        "model.add(Dense(num_classes, input_dim=num_pixels, \n",
        "                 kernel_initializer='normal', activation='softmax'))\n",
        "# Compile model\n",
        "model.compile(loss='categorical_crossentropy', \n",
        "              optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
        "          epochs=20, batch_size=200, verbose=2)\n",
        "\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "784\n",
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_54 (Dense)             (None, 10)                7850      \n",
            "=================================================================\n",
            "Total params: 7,850\n",
            "Trainable params: 7,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/20\n",
            "300/300 - 1s - loss: 0.7701 - accuracy: 0.8157 - val_loss: 0.4297 - val_accuracy: 0.8935\n",
            "Epoch 2/20\n",
            "300/300 - 1s - loss: 0.3990 - accuracy: 0.8951 - val_loss: 0.3445 - val_accuracy: 0.9084\n",
            "Epoch 3/20\n",
            "300/300 - 1s - loss: 0.3434 - accuracy: 0.9069 - val_loss: 0.3145 - val_accuracy: 0.9155\n",
            "Epoch 4/20\n",
            "300/300 - 1s - loss: 0.3177 - accuracy: 0.9128 - val_loss: 0.2989 - val_accuracy: 0.9179\n",
            "Epoch 5/20\n",
            "300/300 - 1s - loss: 0.3022 - accuracy: 0.9168 - val_loss: 0.2878 - val_accuracy: 0.9202\n",
            "Epoch 6/20\n",
            "300/300 - 1s - loss: 0.2916 - accuracy: 0.9190 - val_loss: 0.2824 - val_accuracy: 0.9222\n",
            "Epoch 7/20\n",
            "300/300 - 1s - loss: 0.2842 - accuracy: 0.9209 - val_loss: 0.2774 - val_accuracy: 0.9233\n",
            "Epoch 8/20\n",
            "300/300 - 1s - loss: 0.2783 - accuracy: 0.9227 - val_loss: 0.2737 - val_accuracy: 0.9243\n",
            "Epoch 9/20\n",
            "300/300 - 1s - loss: 0.2738 - accuracy: 0.9242 - val_loss: 0.2707 - val_accuracy: 0.9246\n",
            "Epoch 10/20\n",
            "300/300 - 1s - loss: 0.2699 - accuracy: 0.9247 - val_loss: 0.2677 - val_accuracy: 0.9260\n",
            "Epoch 11/20\n",
            "300/300 - 1s - loss: 0.2664 - accuracy: 0.9259 - val_loss: 0.2672 - val_accuracy: 0.9256\n",
            "Epoch 12/20\n",
            "300/300 - 1s - loss: 0.2639 - accuracy: 0.9265 - val_loss: 0.2695 - val_accuracy: 0.9246\n",
            "Epoch 13/20\n",
            "300/300 - 1s - loss: 0.2615 - accuracy: 0.9271 - val_loss: 0.2659 - val_accuracy: 0.9267\n",
            "Epoch 14/20\n",
            "300/300 - 1s - loss: 0.2590 - accuracy: 0.9280 - val_loss: 0.2653 - val_accuracy: 0.9253\n",
            "Epoch 15/20\n",
            "300/300 - 1s - loss: 0.2574 - accuracy: 0.9287 - val_loss: 0.2659 - val_accuracy: 0.9261\n",
            "Epoch 16/20\n",
            "300/300 - 1s - loss: 0.2558 - accuracy: 0.9290 - val_loss: 0.2642 - val_accuracy: 0.9268\n",
            "Epoch 17/20\n",
            "300/300 - 1s - loss: 0.2543 - accuracy: 0.9297 - val_loss: 0.2640 - val_accuracy: 0.9262\n",
            "Epoch 18/20\n",
            "300/300 - 1s - loss: 0.2526 - accuracy: 0.9299 - val_loss: 0.2629 - val_accuracy: 0.9278\n",
            "Epoch 19/20\n",
            "300/300 - 1s - loss: 0.2512 - accuracy: 0.9304 - val_loss: 0.2624 - val_accuracy: 0.9274\n",
            "Epoch 20/20\n",
            "300/300 - 1s - loss: 0.2501 - accuracy: 0.9308 - val_loss: 0.2622 - val_accuracy: 0.9283\n",
            "Baseline Error: 7.17%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_ornWffFkgn"
      },
      "source": [
        "\n",
        " Model A"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUU9rAWAMvhR",
        "outputId": "cceea029-f666-42c3-d8b8-4b0e8437c3e0"
      },
      "source": [
        "\n",
        "# A simple Dense NN model with \n",
        "# a hidden dense layer of 784 neurons, densely connected to the input layer (of 784 neurons)\n",
        "# densely connected to the output layer of 10 neurons (denoting the 10 digit classes)\n",
        "# \n",
        "\n",
        "import numpy\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "# fix random seed for reproducibility\n",
        "seed = 88\n",
        "numpy.random.seed(seed)\n",
        "# load data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# flatten 28*28 images to a 784 vector for each image\n",
        "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
        "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n",
        "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\n",
        "\n",
        "# normalize inputs from 0-255 to 0-1\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "# one hot encode outputs\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]\n",
        "print(num_classes)\n",
        "print(num_pixels)\n",
        "\n",
        "\n",
        "# build the model\n",
        "# create model\n",
        "model = Sequential()\n",
        "# this is the first hidden layer (784 nodes)\n",
        "model.add(Dense(num_pixels, input_dim=num_pixels, \n",
        "                 kernel_initializer='normal', activation='relu'))\n",
        "# this is the output layer (10 neurons, denoting the 10 digit classes)\n",
        "model.add(Dense(num_classes, \n",
        "                 kernel_initializer='normal', activation='softmax'))\n",
        "# Compile model\n",
        "model.compile(loss='categorical_crossentropy', \n",
        "              optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
        "          epochs=20, batch_size=200, verbose=2)\n",
        "\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "784\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_33 (Dense)             (None, 784)               615440    \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 10)                7850      \n",
            "=================================================================\n",
            "Total params: 623,290\n",
            "Trainable params: 623,290\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/20\n",
            "300/300 - 4s - loss: 0.2808 - accuracy: 0.9205 - val_loss: 0.1412 - val_accuracy: 0.9595\n",
            "Epoch 2/20\n",
            "300/300 - 4s - loss: 0.1088 - accuracy: 0.9690 - val_loss: 0.0929 - val_accuracy: 0.9716\n",
            "Epoch 3/20\n",
            "300/300 - 4s - loss: 0.0705 - accuracy: 0.9794 - val_loss: 0.0793 - val_accuracy: 0.9767\n",
            "Epoch 4/20\n",
            "300/300 - 4s - loss: 0.0482 - accuracy: 0.9863 - val_loss: 0.0732 - val_accuracy: 0.9773\n",
            "Epoch 5/20\n",
            "300/300 - 4s - loss: 0.0355 - accuracy: 0.9900 - val_loss: 0.0648 - val_accuracy: 0.9806\n",
            "Epoch 6/20\n",
            "300/300 - 4s - loss: 0.0263 - accuracy: 0.9931 - val_loss: 0.0622 - val_accuracy: 0.9801\n",
            "Epoch 7/20\n",
            "300/300 - 4s - loss: 0.0195 - accuracy: 0.9951 - val_loss: 0.0578 - val_accuracy: 0.9813\n",
            "Epoch 8/20\n",
            "300/300 - 4s - loss: 0.0149 - accuracy: 0.9966 - val_loss: 0.0583 - val_accuracy: 0.9821\n",
            "Epoch 9/20\n",
            "300/300 - 4s - loss: 0.0104 - accuracy: 0.9979 - val_loss: 0.0581 - val_accuracy: 0.9819\n",
            "Epoch 10/20\n",
            "300/300 - 4s - loss: 0.0074 - accuracy: 0.9988 - val_loss: 0.0560 - val_accuracy: 0.9826\n",
            "Epoch 11/20\n",
            "300/300 - 4s - loss: 0.0056 - accuracy: 0.9992 - val_loss: 0.0578 - val_accuracy: 0.9844\n",
            "Epoch 12/20\n",
            "300/300 - 4s - loss: 0.0044 - accuracy: 0.9993 - val_loss: 0.0621 - val_accuracy: 0.9819\n",
            "Epoch 13/20\n",
            "300/300 - 4s - loss: 0.0038 - accuracy: 0.9995 - val_loss: 0.0634 - val_accuracy: 0.9828\n",
            "Epoch 14/20\n",
            "300/300 - 4s - loss: 0.0028 - accuracy: 0.9997 - val_loss: 0.0636 - val_accuracy: 0.9828\n",
            "Epoch 15/20\n",
            "300/300 - 4s - loss: 0.0064 - accuracy: 0.9983 - val_loss: 0.0682 - val_accuracy: 0.9815\n",
            "Epoch 16/20\n",
            "300/300 - 4s - loss: 0.0123 - accuracy: 0.9962 - val_loss: 0.0807 - val_accuracy: 0.9778\n",
            "Epoch 17/20\n",
            "300/300 - 4s - loss: 0.0059 - accuracy: 0.9984 - val_loss: 0.0610 - val_accuracy: 0.9845\n",
            "Epoch 18/20\n",
            "300/300 - 4s - loss: 0.0019 - accuracy: 0.9998 - val_loss: 0.0590 - val_accuracy: 0.9850\n",
            "Epoch 19/20\n",
            "300/300 - 4s - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.0617 - val_accuracy: 0.9839\n",
            "Epoch 20/20\n",
            "300/300 - 4s - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0691 - val_accuracy: 0.9831\n",
            "Baseline Error: 1.69%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZR27Yhiuman"
      },
      "source": [
        "# **This simple baseline Model A (of one hidden layer) has accuracy of 98.3%**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJLMuMtAFyHQ"
      },
      "source": [
        ""
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQ50Ix1pFyYn"
      },
      "source": [
        ""
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6scTNRCAGB_L"
      },
      "source": [
        "\n",
        " Model B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpT275lyNHNB"
      },
      "source": [
        "\n",
        "# We try a more complicated model, by adding an extra Dense hidden layer of 300 neurons\n",
        "#\n",
        "# fix random seed for reproducibility\n",
        "seed = 88\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# load data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# flatten 28*28 images to a 784 vector for each image\n",
        "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
        "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n",
        "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\n",
        "\n",
        "# normalize inputs from 0-255 to 0-1\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "# one hot encode outputs\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]\n",
        "print(num_classes)\n",
        "print(num_pixels)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "# this is the first hidden layer, as usual\n",
        "model.add(Dense(num_pixels, input_dim=num_pixels, \n",
        "                 kernel_initializer='normal', activation='relu'))\n",
        "# this is the second Dense hidden layer of 300 neurons \n",
        "model.add(Dense(300, kernel_initializer='normal', activation='relu'))\n",
        "# this is the usual output layer of 10 neurons \n",
        "model.add(Dense(num_classes, \n",
        "                 kernel_initializer='normal', activation='softmax'))\n",
        "# Compile model\n",
        "model.compile(loss='categorical_crossentropy', \n",
        "              optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
        "          epochs=20, batch_size=200, verbose=2)\n",
        "\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-H2rYrxH9DG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Xrs2BIRH9Ku"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GVfTZgw0Q_z"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# load MNIST dataset\n",
        "from keras.datasets import mnist\n",
        "\n",
        "\n",
        "\n",
        "#################################################\n",
        "# Step 1:\n",
        "# Load the image data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "print(X_train)\n",
        "print(y_train)\n",
        "print(X_test)\n",
        "print(y_test)\n",
        "\"\"\"\n",
        "X_train:\n",
        "\n",
        "We can play around with the loaded images a bit\n",
        "We will see that X_train is (60000, 28, 28), so that we have\n",
        "60000 training images, each with a 28*28 resolution\n",
        "\n",
        "y_train:\n",
        "This is an array of labels. For example - y_train[:20] is\n",
        "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9],\n",
        "      dtype=uint8)\n",
        "\n",
        "Each label shows what digit the corresponding image in X_train is.\n",
        "\"\"\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oc8w0-BW00wX"
      },
      "source": [
        "\n",
        "#################################################\n",
        "# Step 2:\n",
        "# Pre-processing the data into desired format\n",
        "\n",
        "# reshape into (batch, height, width, channels)\n",
        "# we have 60000 training images and 10000 testing images\n",
        "X_train = X_train.reshape(60000, 28, 28, 1)\n",
        "X_test = X_test.reshape(10000, 28, 28, 1)\n",
        "print(X_train)\n",
        "print(X_test)\n",
        "# Normalize to float between 0 and 1\n",
        "# Original pixel values are between 0 and 255\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "print(X_train)\n",
        "print(X_test)\n",
        "\"\"\"\n",
        "For y_ label data, we want to convert them into categorical / one-hot variables.\n",
        "In our case, we have 10 categories, so for example, for our first label in\n",
        "y_train, the value is 5, then we want to convert it into\n",
        "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0], with the index of 1 indicating the value of y_train[0]\n",
        "\"\"\"\n",
        "\n",
        "# keras.utils.np_utils conveniently has this built in\n",
        "classes = 10\n",
        "y_train = np_utils.to_categorical(y_train, classes)\n",
        "y_test = np_utils.to_categorical(y_test, classes)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTOmwev9MARX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCOOe6qs1xmL"
      },
      "source": [
        "print(y_train[24])\n",
        "xx = np.reshape(X_train[24],(28,28))\n",
        "plt.imshow(xx, cmap=plt.get_cmap('gray'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vjLDqfFm9Lz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B60oylA56UOH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YEgd1Z56UT5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWOxrL1b5SOu"
      },
      "source": [
        "# **Try using a Convolutional model** \n",
        "\n",
        "Try replacing the dense hidden layer of model A with a conv layer\n",
        "\n",
        "Lets call this Model C\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUiM128ymysE",
        "outputId": "f4d1d110-65b5-433c-a6fb-70f450c74259"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Prepare the data\n",
        "# Model / data parameters\n",
        "num_classes = 10\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "# Make sure images have shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(x_train.shape[0], \"train samples\")\n",
        "print(x_test.shape[0], \"test samples\")\n",
        "# x_train shape: (60000, 28, 28, 1)\n",
        "# 60000 train samples\n",
        "# 10000 test samples\n",
        "\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# Build the model\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        # first hidden layer, which is convolutional\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        # output layer of 10 neurons\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "batch_size = 128\n",
        "epochs = 18\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "\n",
        "# Evaluate the trained model\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 10816)             0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 10816)             0         \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 10)                108170    \n",
            "=================================================================\n",
            "Total params: 108,810\n",
            "Trainable params: 108,810\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/18\n",
            "422/422 [==============================] - 32s 76ms/step - loss: 0.3457 - accuracy: 0.9046 - val_loss: 0.1174 - val_accuracy: 0.9692\n",
            "Epoch 2/18\n",
            "422/422 [==============================] - 33s 78ms/step - loss: 0.1294 - accuracy: 0.9627 - val_loss: 0.0809 - val_accuracy: 0.9802\n",
            "Epoch 3/18\n",
            "422/422 [==============================] - 33s 78ms/step - loss: 0.0982 - accuracy: 0.9716 - val_loss: 0.0750 - val_accuracy: 0.9805\n",
            "Epoch 4/18\n",
            "422/422 [==============================] - 33s 78ms/step - loss: 0.0854 - accuracy: 0.9745 - val_loss: 0.0650 - val_accuracy: 0.9823\n",
            "Epoch 5/18\n",
            "422/422 [==============================] - 33s 77ms/step - loss: 0.0750 - accuracy: 0.9775 - val_loss: 0.0608 - val_accuracy: 0.9837\n",
            "Epoch 6/18\n",
            "422/422 [==============================] - 33s 77ms/step - loss: 0.0709 - accuracy: 0.9784 - val_loss: 0.0574 - val_accuracy: 0.9845\n",
            "Epoch 7/18\n",
            "422/422 [==============================] - 33s 77ms/step - loss: 0.0653 - accuracy: 0.9798 - val_loss: 0.0545 - val_accuracy: 0.9847\n",
            "Epoch 8/18\n",
            "422/422 [==============================] - 33s 77ms/step - loss: 0.0619 - accuracy: 0.9811 - val_loss: 0.0537 - val_accuracy: 0.9848\n",
            "Epoch 9/18\n",
            "422/422 [==============================] - 33s 77ms/step - loss: 0.0566 - accuracy: 0.9824 - val_loss: 0.0541 - val_accuracy: 0.9855\n",
            "Epoch 10/18\n",
            "422/422 [==============================] - 33s 77ms/step - loss: 0.0549 - accuracy: 0.9831 - val_loss: 0.0529 - val_accuracy: 0.9862\n",
            "Epoch 11/18\n",
            "422/422 [==============================] - 33s 78ms/step - loss: 0.0515 - accuracy: 0.9836 - val_loss: 0.0508 - val_accuracy: 0.9870\n",
            "Epoch 12/18\n",
            "422/422 [==============================] - 33s 78ms/step - loss: 0.0490 - accuracy: 0.9844 - val_loss: 0.0499 - val_accuracy: 0.9855\n",
            "Epoch 13/18\n",
            "422/422 [==============================] - 33s 77ms/step - loss: 0.0459 - accuracy: 0.9850 - val_loss: 0.0489 - val_accuracy: 0.9873\n",
            "Epoch 14/18\n",
            "422/422 [==============================] - 33s 78ms/step - loss: 0.0440 - accuracy: 0.9862 - val_loss: 0.0528 - val_accuracy: 0.9863\n",
            "Epoch 15/18\n",
            "422/422 [==============================] - 33s 77ms/step - loss: 0.0424 - accuracy: 0.9869 - val_loss: 0.0552 - val_accuracy: 0.9865\n",
            "Epoch 16/18\n",
            "422/422 [==============================] - 33s 77ms/step - loss: 0.0408 - accuracy: 0.9869 - val_loss: 0.0530 - val_accuracy: 0.9863\n",
            "Epoch 17/18\n",
            "422/422 [==============================] - 32s 77ms/step - loss: 0.0388 - accuracy: 0.9869 - val_loss: 0.0494 - val_accuracy: 0.9868\n",
            "Epoch 18/18\n",
            "422/422 [==============================] - 32s 76ms/step - loss: 0.0367 - accuracy: 0.9883 - val_loss: 0.0506 - val_accuracy: 0.9863\n",
            "Test loss: 0.047353312373161316\n",
            "Test accuracy: 0.98580002784729\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yIkhf5B9nDB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjmc1-yO9pNh"
      },
      "source": [
        "# **This is Model D**\n",
        "We are adding one more convolutional layer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R28Dm62h9njE",
        "outputId": "24ebb999-a137-4df7-a4b0-8495370042c3"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Prepare the data\n",
        "# Model / data parameters\n",
        "num_classes = 10\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "# Make sure images have shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(x_train.shape[0], \"train samples\")\n",
        "print(x_test.shape[0], \"test samples\")\n",
        "# x_train shape: (60000, 28, 28, 1)\n",
        "# 60000 train samples\n",
        "# 10000 test samples\n",
        "\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# Build the model\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        # first hidden layer, which is convolutional, 32 filters\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        # second hidden layer, which is convolutional, 64 filters\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        # output layer of 10 neurons\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "batch_size = 128\n",
        "epochs = 18\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "\n",
        "# Evaluate the trained model\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential_24\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_58 (Dense)             (None, 10)                16010     \n",
            "=================================================================\n",
            "Total params: 53,578\n",
            "Trainable params: 53,578\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/18\n",
            "422/422 [==============================] - 63s 149ms/step - loss: 0.3448 - accuracy: 0.8916 - val_loss: 0.0776 - val_accuracy: 0.9785\n",
            "Epoch 2/18\n",
            "422/422 [==============================] - 63s 148ms/step - loss: 0.1071 - accuracy: 0.9669 - val_loss: 0.0558 - val_accuracy: 0.9845\n",
            "Epoch 3/18\n",
            "422/422 [==============================] - 63s 149ms/step - loss: 0.0802 - accuracy: 0.9751 - val_loss: 0.0437 - val_accuracy: 0.9872\n",
            "Epoch 4/18\n",
            "422/422 [==============================] - 67s 159ms/step - loss: 0.0686 - accuracy: 0.9787 - val_loss: 0.0410 - val_accuracy: 0.9887\n",
            "Epoch 5/18\n",
            "422/422 [==============================] - 63s 150ms/step - loss: 0.0601 - accuracy: 0.9811 - val_loss: 0.0396 - val_accuracy: 0.9885\n",
            "Epoch 6/18\n",
            "422/422 [==============================] - 63s 150ms/step - loss: 0.0542 - accuracy: 0.9830 - val_loss: 0.0361 - val_accuracy: 0.9892\n",
            "Epoch 7/18\n",
            "422/422 [==============================] - 63s 149ms/step - loss: 0.0499 - accuracy: 0.9843 - val_loss: 0.0317 - val_accuracy: 0.9902\n",
            "Epoch 8/18\n",
            "422/422 [==============================] - 63s 149ms/step - loss: 0.0451 - accuracy: 0.9856 - val_loss: 0.0342 - val_accuracy: 0.9902\n",
            "Epoch 9/18\n",
            "422/422 [==============================] - 63s 149ms/step - loss: 0.0422 - accuracy: 0.9870 - val_loss: 0.0356 - val_accuracy: 0.9893\n",
            "Epoch 10/18\n",
            "422/422 [==============================] - 63s 149ms/step - loss: 0.0384 - accuracy: 0.9878 - val_loss: 0.0295 - val_accuracy: 0.9920\n",
            "Epoch 11/18\n",
            "422/422 [==============================] - 63s 150ms/step - loss: 0.0369 - accuracy: 0.9881 - val_loss: 0.0288 - val_accuracy: 0.9930\n",
            "Epoch 12/18\n",
            "422/422 [==============================] - 63s 150ms/step - loss: 0.0346 - accuracy: 0.9889 - val_loss: 0.0281 - val_accuracy: 0.9917\n",
            "Epoch 13/18\n",
            "422/422 [==============================] - 62s 148ms/step - loss: 0.0328 - accuracy: 0.9892 - val_loss: 0.0269 - val_accuracy: 0.9927\n",
            "Epoch 14/18\n",
            "422/422 [==============================] - 69s 164ms/step - loss: 0.0318 - accuracy: 0.9898 - val_loss: 0.0287 - val_accuracy: 0.9918\n",
            "Epoch 15/18\n",
            "422/422 [==============================] - 62s 147ms/step - loss: 0.0299 - accuracy: 0.9903 - val_loss: 0.0280 - val_accuracy: 0.9927\n",
            "Epoch 16/18\n",
            "422/422 [==============================] - 62s 148ms/step - loss: 0.0291 - accuracy: 0.9902 - val_loss: 0.0267 - val_accuracy: 0.9933\n",
            "Epoch 17/18\n",
            "422/422 [==============================] - 62s 148ms/step - loss: 0.0289 - accuracy: 0.9905 - val_loss: 0.0263 - val_accuracy: 0.9933\n",
            "Epoch 18/18\n",
            "422/422 [==============================] - 62s 147ms/step - loss: 0.0261 - accuracy: 0.9911 - val_loss: 0.0255 - val_accuracy: 0.9930\n",
            "Test loss: 0.02506801299750805\n",
            "Test accuracy: 0.9921000003814697\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyNLwuRt6We2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITebD1k06Wiw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld_kxjix6WmD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IDRvb6lGfw7"
      },
      "source": [
        "# **This is Model E**\n",
        "We add on top of Model D, a dense layer of 512 neurons after the 2 convolutional layers. \n",
        "\n",
        "# **I also create a new Model F**\n",
        "Which uses the Keras ImageDataGenerator() API to augment the training data with data that are shifted rotated etc. It also uses model.fit_generator() to train. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWkdS9JH07Hl"
      },
      "source": [
        "# load MNIST dataset\n",
        "from keras.datasets import mnist\n",
        "\n",
        "#################################################\n",
        "# Step 1:\n",
        "# Load the image data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#################################################\n",
        "# Step 2:\n",
        "# Pre-processing the data into desired format\n",
        "\n",
        "# reshape into (batch, height, width, channels)\n",
        "# we have 60000 training images and 10000 testing images\n",
        "X_train = X_train.reshape(60000, 28, 28, 1)\n",
        "X_test = X_test.reshape(10000, 28, 28, 1)\n",
        "print(X_train)\n",
        "print(X_test)\n",
        "# Normalize to float between 0 and 1\n",
        "# Original pixel values are between 0 and 255\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "print(X_train)\n",
        "print(X_test)\n",
        "\n",
        "# keras.utils.np_utils conveniently has this built in\n",
        "classes = 10\n",
        "y_train = np_utils.to_categorical(y_train, classes)\n",
        "y_test = np_utils.to_categorical(y_test, classes)\n",
        "\n",
        "\n",
        "#################################################\n",
        "# Step 3:\n",
        "# Build our Neural Network\n",
        "\n",
        "# As earlier, but with much lower dimensions and fewer layers\n",
        "# (since our input is 28*28 images anyway)\n",
        "# Instantiate a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# 1st Convolutional Layer\n",
        "model.add(Conv2D(filters=32, input_shape=(28, 28, 1), kernel_size=(3, 3), strides=(1, 1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "# Max Pooling\n",
        "model.add(MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='valid'))\n",
        "\n",
        "\n",
        "\n",
        "# 2nd Convolutional Layer\n",
        "model.add(Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# Max Pooling\n",
        "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='valid'))\n",
        "\n",
        "# Fully Connected layer\n",
        "model.add(Flatten())\n",
        "# 1st Fully Connected Layer\n",
        "model.add(Dense(512)) \n",
        "model.add(Activation('relu'))\n",
        "# Add Dropout to prevent overfitting\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Output Layer\n",
        "# important to have dense 10, since we have 10 classes\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JwHC3Oo1aC5"
      },
      "source": [
        "\n",
        "\n",
        "#################################################\n",
        "# Step 4:\n",
        "# Prepare for training\n",
        "\n",
        "# We use ImageDataGenerator to augment our input data\n",
        "# which among other benefits, can help reduce over-fitting\n",
        "gen = ImageDataGenerator(\n",
        "            rotation_range=8,\n",
        "            width_shift_range=0.08,\n",
        "            shear_range=0.3,\n",
        "            height_shift_range=0.08,\n",
        "            zoom_range=0.08\n",
        "            )\n",
        "test_gen = ImageDataGenerator()\n",
        "\n",
        "# hyoer-parameters\n",
        "# We train in batches to speed up the process\n",
        "# (and so that our memory can handle the data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "# How many rounds of training? Let's start from a smaller number\n",
        "EPOCHS = 5\n",
        "\n",
        "# Generator to \"flow\" in the input images and labels into our model\n",
        "# Takes batch_size as a parameter\n",
        "train_generator = gen.flow(X_train, y_train, batch_size=BATCH_SIZE)\n",
        "test_generator = test_gen.flow(X_test, y_test, batch_size=BATCH_SIZE)\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnPTWKT-1ckk",
        "outputId": "f626c566-5006-4853-ae2c-0607e8114f61"
      },
      "source": [
        "\n",
        "#################################################\n",
        "# Step 5:\n",
        "# Do the training!\n",
        "model.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=60000//BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=test_generator,\n",
        "        validation_steps=10000//BATCH_SIZE\n",
        "        )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-49-a5f943823347>:10: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/5\n",
            "937/937 [==============================] - 183s 196ms/step - loss: 0.2019 - accuracy: 0.9366 - val_loss: 0.0342 - val_accuracy: 0.9888\n",
            "Epoch 2/5\n",
            "937/937 [==============================] - 183s 195ms/step - loss: 0.0793 - accuracy: 0.9753 - val_loss: 0.0394 - val_accuracy: 0.9873\n",
            "Epoch 3/5\n",
            "937/937 [==============================] - 181s 194ms/step - loss: 0.0596 - accuracy: 0.9812 - val_loss: 0.0273 - val_accuracy: 0.9915\n",
            "Epoch 4/5\n",
            "186/937 [====>.........................] - ETA: 2:20 - loss: 0.0575 - accuracy: 0.9821"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPLmJRcEBwS4",
        "outputId": "fc2c4565-237d-40e0-b7ac-beb3cd318d04"
      },
      "source": [
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.021829180419445038\n",
            "Test accuracy: 0.992900013923645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUSaaXbg1exB"
      },
      "source": [
        "\n",
        "#################################################\n",
        "# Step 6:\n",
        "# Do some predictions\n",
        "\n",
        "# Ideally we would want to use some new iamges to play with predictions\n",
        "# but lets just grab some from the test set\n",
        "\n",
        "# Take first image in testing dataset\n",
        "to_predict = X_test[0]\n",
        "# reshape from (28, 28, 1) --> (1, 28, 28, 1)\n",
        "to_predict = np.expand_dims(to_predict, axis=0)\n",
        "\n",
        "# make prediction: this gives probability distribution\n",
        "# over all classes\n",
        "model.predict(to_predict)\n",
        "\n",
        "# This returns the class with the highest probability\n",
        "print(model.predict_classes(to_predict))\n",
        "# --> array([7], dtype=int64)\n",
        "\n",
        "# Let's compare with actual label:\n",
        "y_test[0]\n",
        "# array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], dtype=float32)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZjRrD8j27O_"
      },
      "source": [
        "print(y_test[0])\n",
        "xx = np.reshape(X_test[0],(28,28))\n",
        "plt.imshow(xx, cmap=plt.get_cmap('gray'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MciPaA3oHwu8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERu1hVACHwyJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8xa2DQb7tUG",
        "outputId": "fc66a39e-1246-46e0-b1a0-cfd2aa38cc05"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# 1st Convolutional Layer\n",
        "model.add(Conv2D(filters=32, input_shape=(28, 28, 1), kernel_size=(3, 3), strides=(1, 1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "# Max Pooling\n",
        "model.add(MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='valid'))\n",
        "\n",
        "\n",
        "\n",
        "# 2nd Convolutional Layer\n",
        "model.add(Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# Max Pooling\n",
        "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='valid'))\n",
        "\n",
        "# Fully Connected layer\n",
        "model.add(Flatten())\n",
        "# 1st Fully Connected Layer\n",
        "model.add(Dense(512)) \n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(256)) \n",
        "model.add(Activation('relu'))\n",
        "# Add Dropout to prevent overfitting\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Output Layer\n",
        "# important to have dense 10, since we have 10 classes\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 26, 26, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 22, 22, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 22, 22, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 20, 20, 128)       73856     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 20, 20, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 9, 9, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 10368)             0         \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 512)               5308928   \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_60 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_61 (Dense)             (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 5,535,498\n",
            "Trainable params: 5,535,498\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9EmNJKT71LV"
      },
      "source": [
        "gen = ImageDataGenerator(\n",
        "            rotation_range=10,\n",
        "            width_shift_range=0.1,\n",
        "            shear_range=0.5,\n",
        "            height_shift_range=0.1,\n",
        "            zoom_range=0.1\n",
        "            )\n",
        "test_gen = ImageDataGenerator()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "5ZfTlpdz74qN",
        "outputId": "11789a32-3159-4b85-d369-e3af128f7816"
      },
      "source": [
        "model.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=60000//BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=test_generator,\n",
        "        validation_steps=10000//BATCH_SIZE\n",
        "        )"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-6b7cf419ef31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit_generator(\n\u001b[0;32m----> 2\u001b[0;31m         \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60000\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_generator' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77XFCmHWye_G",
        "outputId": "8fe08420-fa34-4ffc-ad5c-1c43a75be326"
      },
      "source": [
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.0207768976688385\n",
            "Test accuracy: 0.9940999746322632\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWKMHRg7HZNl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDQxFrUsHZQt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbbNYauwHZVX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dwCE95x8oKe"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# 1st Convolutional Layer\n",
        "model.add(Conv2D(filters=32, input_shape=(28, 28, 1), kernel_size=(3, 3), strides=(1, 1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "# Max Pooling\n",
        "model.add(MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='valid'))\n",
        "\n",
        "\n",
        "\n",
        "# 2nd Convolutional Layer\n",
        "model.add(Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# Max Pooling\n",
        "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='valid'))\n",
        "\n",
        "# Fully Connected layer\n",
        "model.add(Flatten())\n",
        "# 1st Fully Connected Layer\n",
        "model.add(Dense(512)) \n",
        "model.add(Activation('relu'))\n",
        "model = Sequential()\n",
        "\n",
        "# 1st Convolutional Layer\n",
        "model.add(Conv2D(filters=32, input_shape=(28, 28, 1), kernel_size=(3, 3), strides=(1, 1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "# Max Pooling\n",
        "model.add(MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='valid'))\n",
        "\n",
        "\n",
        "\n",
        "# 2nd Convolutional Layer\n",
        "model.add(Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# Max Pooling\n",
        "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='valid'))\n",
        "\n",
        "# Fully Connected layer\n",
        "model.add(Flatten())\n",
        "# 1st Fully Connected Layer\n",
        "model.add(Dense(512)) \n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(256)) \n",
        "\n",
        "model.add(Activation('relu'))\n",
        "# Add Dropout to prevent overfitting\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Output Layer\n",
        "# important to have dense 10, since we have 10 classes\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKUBwQDk82kx"
      },
      "source": [
        "model.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=60000//BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=test_generator,\n",
        "        validation_steps=10000//BATCH_SIZE\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKZ3xyHr1hNf"
      },
      "source": [
        "\n",
        "#################################################\n",
        "# Step 7:\n",
        "# Save your model\n",
        "\n",
        "\"\"\"\n",
        "Although this particular model was pretty fast to train,\n",
        "sometimes our model would actually take a long time to train,\n",
        "and it would be quite disappointing if we need to retrain these models\n",
        "everytime we use them, yes?\n",
        "\n",
        "Thankfully, we can easily save and reload them whenever we need.\n",
        "\"\"\"\n",
        "from keras.models import load_model\n",
        "\n",
        "model.save('my_mnist_model.h5')\n",
        "\n",
        "# When we need to load it back\n",
        "# we can just run\n",
        "model = load_model('my_mnist_model.h5')\n",
        "# and get the same model object as earlier.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qi_0WFvHCmUk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L-zpzOvHsyJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qn8XVp77Cmd5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJZ_W6rzCmm7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZsPS0eM-9Vu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq8a18Zs-3sL"
      },
      "source": [
        "# Try using a simpler CNN model \n",
        "# with 2 conv layer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16KcZw6pCp9Y",
        "outputId": "7e3408aa-7481-4a71-fbb3-e5b009dc5ec4"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Prepare the data\n",
        "# Model / data parameters\n",
        "num_classes = 10\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "# Make sure images have shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(x_train.shape[0], \"train samples\")\n",
        "print(x_test.shape[0], \"test samples\")\n",
        "# x_train shape: (60000, 28, 28, 1)\n",
        "# 60000 train samples\n",
        "# 10000 test samples\n",
        "\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# Build the model\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "batch_size = 128\n",
        "epochs = 18\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "\n",
        "# Evaluate the trained model\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_14 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 10)                16010     \n",
            "=================================================================\n",
            "Total params: 34,826\n",
            "Trainable params: 34,826\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/18\n",
            "422/422 [==============================] - 34s 81ms/step - loss: 0.3723 - accuracy: 0.8885 - val_loss: 0.0862 - val_accuracy: 0.9760\n",
            "Epoch 2/18\n",
            "422/422 [==============================] - 34s 80ms/step - loss: 0.1132 - accuracy: 0.9659 - val_loss: 0.0594 - val_accuracy: 0.9842\n",
            "Epoch 3/18\n",
            "422/422 [==============================] - 34s 80ms/step - loss: 0.0864 - accuracy: 0.9739 - val_loss: 0.0502 - val_accuracy: 0.9862\n",
            "Epoch 4/18\n",
            "422/422 [==============================] - 34s 81ms/step - loss: 0.0715 - accuracy: 0.9777 - val_loss: 0.0427 - val_accuracy: 0.9890\n",
            "Epoch 5/18\n",
            "422/422 [==============================] - 34s 80ms/step - loss: 0.0641 - accuracy: 0.9799 - val_loss: 0.0388 - val_accuracy: 0.9892\n",
            "Epoch 6/18\n",
            "422/422 [==============================] - 37s 87ms/step - loss: 0.0574 - accuracy: 0.9825 - val_loss: 0.0377 - val_accuracy: 0.9902\n",
            "Epoch 7/18\n",
            "422/422 [==============================] - 34s 81ms/step - loss: 0.0533 - accuracy: 0.9827 - val_loss: 0.0365 - val_accuracy: 0.9900\n",
            "Epoch 8/18\n",
            "422/422 [==============================] - 34s 80ms/step - loss: 0.0479 - accuracy: 0.9845 - val_loss: 0.0342 - val_accuracy: 0.9900\n",
            "Epoch 9/18\n",
            "422/422 [==============================] - 34s 80ms/step - loss: 0.0467 - accuracy: 0.9850 - val_loss: 0.0323 - val_accuracy: 0.9915\n",
            "Epoch 10/18\n",
            "422/422 [==============================] - 34s 80ms/step - loss: 0.0435 - accuracy: 0.9860 - val_loss: 0.0322 - val_accuracy: 0.9903\n",
            "Epoch 11/18\n",
            "422/422 [==============================] - 34s 80ms/step - loss: 0.0384 - accuracy: 0.9876 - val_loss: 0.0316 - val_accuracy: 0.9915\n",
            "Epoch 12/18\n",
            "422/422 [==============================] - 34s 80ms/step - loss: 0.0370 - accuracy: 0.9886 - val_loss: 0.0306 - val_accuracy: 0.9910\n",
            "Epoch 13/18\n",
            "422/422 [==============================] - 34s 80ms/step - loss: 0.0385 - accuracy: 0.9873 - val_loss: 0.0279 - val_accuracy: 0.9923\n",
            "Epoch 14/18\n",
            "422/422 [==============================] - 34s 80ms/step - loss: 0.0352 - accuracy: 0.9885 - val_loss: 0.0293 - val_accuracy: 0.9913\n",
            "Epoch 15/18\n",
            "422/422 [==============================] - 34s 80ms/step - loss: 0.0340 - accuracy: 0.9892 - val_loss: 0.0289 - val_accuracy: 0.9913\n",
            "Epoch 16/18\n",
            "422/422 [==============================] - 34s 81ms/step - loss: 0.0324 - accuracy: 0.9892 - val_loss: 0.0291 - val_accuracy: 0.9923\n",
            "Epoch 17/18\n",
            "422/422 [==============================] - 34s 80ms/step - loss: 0.0312 - accuracy: 0.9901 - val_loss: 0.0308 - val_accuracy: 0.9908\n",
            "Epoch 18/18\n",
            "422/422 [==============================] - 34s 80ms/step - loss: 0.0319 - accuracy: 0.9895 - val_loss: 0.0258 - val_accuracy: 0.9938\n",
            "Test loss: 0.02246723882853985\n",
            "Test accuracy: 0.9921000003814697\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NIMpEYwGEPq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r028gsehGEXV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pqv0zAMFKYB"
      },
      "source": [
        "Add a Dense 512 after the 2 conv layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qi0sqxQaFI7I",
        "outputId": "2e9c685e-5ea0-4d9a-9113-c7dac2ade703"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Prepare the data\n",
        "# Model / data parameters\n",
        "num_classes = 10\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "# Make sure images have shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(x_train.shape[0], \"train samples\")\n",
        "print(x_test.shape[0], \"test samples\")\n",
        "# x_train shape: (60000, 28, 28, 1)\n",
        "# 60000 train samples\n",
        "# 10000 test samples\n",
        "\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# Build the model\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        # add 1 more dense layer of 512 neurons\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "batch_size = 128\n",
        "epochs = 18\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "\n",
        "# Evaluate the trained model\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential_23\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_26 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_22 (MaxPooling (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_23 (MaxPooling (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "dense_52 (Dense)             (None, 5, 5, 512)         33280     \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 12800)             0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 12800)             0         \n",
            "_________________________________________________________________\n",
            "dense_53 (Dense)             (None, 10)                128010    \n",
            "=================================================================\n",
            "Total params: 180,106\n",
            "Trainable params: 180,106\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/18\n",
            "422/422 [==============================] - 47s 112ms/step - loss: 0.2283 - accuracy: 0.9308 - val_loss: 0.0777 - val_accuracy: 0.9780\n",
            "Epoch 2/18\n",
            "422/422 [==============================] - 47s 112ms/step - loss: 0.0658 - accuracy: 0.9798 - val_loss: 0.0506 - val_accuracy: 0.9852\n",
            "Epoch 3/18\n",
            "422/422 [==============================] - 47s 112ms/step - loss: 0.0496 - accuracy: 0.9853 - val_loss: 0.0419 - val_accuracy: 0.9880\n",
            "Epoch 4/18\n",
            "422/422 [==============================] - 47s 112ms/step - loss: 0.0402 - accuracy: 0.9870 - val_loss: 0.0368 - val_accuracy: 0.9897\n",
            "Epoch 5/18\n",
            "422/422 [==============================] - 47s 112ms/step - loss: 0.0354 - accuracy: 0.9887 - val_loss: 0.0370 - val_accuracy: 0.9885\n",
            "Epoch 6/18\n",
            "422/422 [==============================] - 47s 112ms/step - loss: 0.0305 - accuracy: 0.9899 - val_loss: 0.0342 - val_accuracy: 0.9900\n",
            "Epoch 7/18\n",
            "422/422 [==============================] - 47s 112ms/step - loss: 0.0266 - accuracy: 0.9912 - val_loss: 0.0296 - val_accuracy: 0.9917\n",
            "Epoch 8/18\n",
            "422/422 [==============================] - 47s 112ms/step - loss: 0.0240 - accuracy: 0.9917 - val_loss: 0.0288 - val_accuracy: 0.9917\n",
            "Epoch 9/18\n",
            "422/422 [==============================] - 47s 112ms/step - loss: 0.0200 - accuracy: 0.9935 - val_loss: 0.0332 - val_accuracy: 0.9903\n",
            "Epoch 10/18\n",
            "422/422 [==============================] - 47s 112ms/step - loss: 0.0190 - accuracy: 0.9938 - val_loss: 0.0326 - val_accuracy: 0.9918\n",
            "Epoch 11/18\n",
            "422/422 [==============================] - 47s 112ms/step - loss: 0.0171 - accuracy: 0.9940 - val_loss: 0.0294 - val_accuracy: 0.9928\n",
            "Epoch 12/18\n",
            "422/422 [==============================] - 47s 112ms/step - loss: 0.0161 - accuracy: 0.9949 - val_loss: 0.0288 - val_accuracy: 0.9927\n",
            "Epoch 13/18\n",
            "422/422 [==============================] - 48s 113ms/step - loss: 0.0152 - accuracy: 0.9945 - val_loss: 0.0336 - val_accuracy: 0.9908\n",
            "Epoch 14/18\n",
            "422/422 [==============================] - 48s 113ms/step - loss: 0.0133 - accuracy: 0.9954 - val_loss: 0.0366 - val_accuracy: 0.9913\n",
            "Epoch 15/18\n",
            "422/422 [==============================] - 48s 113ms/step - loss: 0.0130 - accuracy: 0.9956 - val_loss: 0.0403 - val_accuracy: 0.9893\n",
            "Epoch 16/18\n",
            "422/422 [==============================] - 48s 113ms/step - loss: 0.0111 - accuracy: 0.9963 - val_loss: 0.0318 - val_accuracy: 0.9923\n",
            "Epoch 17/18\n",
            "422/422 [==============================] - 48s 113ms/step - loss: 0.0102 - accuracy: 0.9964 - val_loss: 0.0424 - val_accuracy: 0.9917\n",
            "Epoch 18/18\n",
            "422/422 [==============================] - 48s 113ms/step - loss: 0.0096 - accuracy: 0.9966 - val_loss: 0.0441 - val_accuracy: 0.9910\n",
            "Test loss: 0.031723108142614365\n",
            "Test accuracy: 0.9927999973297119\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NtpX7iY_qFb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oyw7RvS1sTOj",
        "outputId": "db3d2d28-7ad5-4f90-c25c-c39f2acedccb"
      },
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.7),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "batch_size = 128\n",
        "epochs = 15\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "\n",
        "# Evaluate the trained model\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_22 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_19 (MaxPooling (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 3, 3, 128)         73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_20 (MaxPooling (None, 1, 1, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 93,962\n",
            "Trainable params: 93,962\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/15\n",
            "422/422 [==============================] - 38s 90ms/step - loss: 0.6762 - accuracy: 0.7866 - val_loss: 0.1208 - val_accuracy: 0.9668\n",
            "Epoch 2/15\n",
            "422/422 [==============================] - 38s 90ms/step - loss: 0.2686 - accuracy: 0.9201 - val_loss: 0.0789 - val_accuracy: 0.9777\n",
            "Epoch 3/15\n",
            "422/422 [==============================] - 38s 90ms/step - loss: 0.2060 - accuracy: 0.9395 - val_loss: 0.0636 - val_accuracy: 0.9803\n",
            "Epoch 4/15\n",
            "422/422 [==============================] - 38s 89ms/step - loss: 0.1681 - accuracy: 0.9519 - val_loss: 0.0588 - val_accuracy: 0.9828\n",
            "Epoch 5/15\n",
            "422/422 [==============================] - 38s 90ms/step - loss: 0.1485 - accuracy: 0.9561 - val_loss: 0.0530 - val_accuracy: 0.9848\n",
            "Epoch 6/15\n",
            "422/422 [==============================] - 38s 90ms/step - loss: 0.1322 - accuracy: 0.9629 - val_loss: 0.0452 - val_accuracy: 0.9883\n",
            "Epoch 7/15\n",
            "422/422 [==============================] - 38s 90ms/step - loss: 0.1206 - accuracy: 0.9647 - val_loss: 0.0440 - val_accuracy: 0.9880\n",
            "Epoch 8/15\n",
            "422/422 [==============================] - 38s 90ms/step - loss: 0.1135 - accuracy: 0.9676 - val_loss: 0.0434 - val_accuracy: 0.9878\n",
            "Epoch 9/15\n",
            "422/422 [==============================] - 38s 89ms/step - loss: 0.1047 - accuracy: 0.9698 - val_loss: 0.0405 - val_accuracy: 0.9887\n",
            "Epoch 10/15\n",
            "422/422 [==============================] - 38s 90ms/step - loss: 0.1010 - accuracy: 0.9716 - val_loss: 0.0382 - val_accuracy: 0.9893\n",
            "Epoch 11/15\n",
            "422/422 [==============================] - 38s 89ms/step - loss: 0.0928 - accuracy: 0.9731 - val_loss: 0.0356 - val_accuracy: 0.9910\n",
            "Epoch 12/15\n",
            "422/422 [==============================] - 38s 89ms/step - loss: 0.0870 - accuracy: 0.9738 - val_loss: 0.0409 - val_accuracy: 0.9875\n",
            "Epoch 13/15\n",
            "422/422 [==============================] - 38s 89ms/step - loss: 0.0838 - accuracy: 0.9754 - val_loss: 0.0414 - val_accuracy: 0.9890\n",
            "Epoch 14/15\n",
            "422/422 [==============================] - 38s 89ms/step - loss: 0.0788 - accuracy: 0.9765 - val_loss: 0.0374 - val_accuracy: 0.9902\n",
            "Epoch 15/15\n",
            "422/422 [==============================] - 38s 90ms/step - loss: 0.0730 - accuracy: 0.9775 - val_loss: 0.0365 - val_accuracy: 0.9910\n",
            "Test loss: 0.04203598573803902\n",
            "Test accuracy: 0.9884999990463257\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}